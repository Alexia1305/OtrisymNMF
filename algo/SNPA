include("NNLS.jl")
function activeset(A::AbstractMatrix{T},
                   b::AbstractVector{T},
                   x0::AbstractVector{T}=Vector{T}(undef, 0);
                   inputgram::Bool=false,
                   outerror::Bool=false,
                   btb::T=0.0, # btb is used only if outerror is true
                   sumtoone::Bool=false,
                   tol::T=1e-12
                   ) where T <: AbstractFloat
    # Init Gram matrices if needed
    AtA = inputgram ? A : A'*A
    Atb = inputgram ? b : A'*b

    # Init constants
    r = size(AtA, 1)
    maxiter = 5*r

    # Init variables
    t = r + 1
    p = 3

    # If not provided, init x full of zeros
    x = (length(x0) == 0) ? zeros(r) : x0
    y = zeros(r)

    # Init support (what Kim calls passive set, ie, entries not constrained to 0)
    F = x .> tol

    # NNLS main loop
    for it in 1:maxiter
        # Solve least squares (x) and compute gradient (y)
        if !sumtoone # standard algorithm
            x[F] = AtA[F,F] \ Atb[F]
            y[.~F] = AtA[.~F,F] * x[F] - Atb[.~F]
        else # modified so that solution has unit l1 norm
            sF = sum(F)
            As = [AtA[F,F] -ones(sF,1) ; ones(1,sF) 0]
            bs = [Atb[F] ; 1]
            sol = As \ bs
            x[F] = sol[1:sF]
            mu = sol[sF+1]
            y[.~F] = AtA[.~F,F] * x[F] - Atb[.~F] .- mu
        end
        x[.~F] .= 0
        y[F] .= 0

        # Compute h1 and h2 (optimality conditions)
        h1 = (x .< -tol) .& F
        h2 = (y .< -tol) .& .~F
        h1c = falses(r)
        h2c = falses(r)

        notgood = sum(h1) + sum(h2)

        # Stopping criterion (if optimality reached)
        if notgood <= 0
            break
        end

        # Update
        if notgood < t
            t = notgood
            p = 3
            h1c = copy(h1)
            h2c = copy(h2)
        end

        if (notgood >= t) & (p >= 1)
            p = p - 1
            h1c = copy(h1)
            h2c = copy(h2)
        end

        if (notgood >= t) & (p == 0)
            tochange = findlast(h1 .| h2)
            if tochange!=nothing && h1[tochange]
                h1c .= false
                h1c[tochange] = true
                h2c .= false
            else
                h1c .= false
                h2c .= false
                if tochange!=nothing
                    h2c[tochange] = true
                end
            end
        end

        F = xor.(F, h1c) .| h2c
    end
    # Remove numeric noise
    x[x .< tol] .= 0
    # Return solution, and if needed also return the residual error
    if outerror
        if iszero(btb)
            error("Keyword argument btb is needed to output the error")
        end
        return x, (btb - Atb[F]' * x[F])
    else
        return x
    end
end


# Solve NNLS constrained to a given support
# This function was intended for use only with the branch-and-bound algo
# so it accepts only the Gram matrices in input
function support_activeset(AtA::AbstractMatrix{T},
                           Atb::AbstractVector{T},
                           btb::T,
                           supp::Vector{Int64},
                           x0::Vector{T}=Vector{T}(undef, 0);
                           sumtoone::Bool=false
                           ) where T <: AbstractFloat
    # Init constant
    r = size(AtA, 1)
    # If not provided, init x0 full of zeros
    x0 = (length(x0) == 0) ? zeros(r) : x0
    # Compute x with activeset restricted to the given support
    x = zeros(r)
    x[supp], resid = activeset(AtA[supp, supp], Atb[supp], x0[supp],
                               inputgram=true, outerror=true, btb=btb,
                               sumtoone=sumtoone)
    # Return solution and residual error
    return x, resid
end


# Solve multiple right-hand side NNLS
function matrixactiveset(X::AbstractMatrix{T},
                         W::AbstractMatrix{T},
                         H0::AbstractMatrix{T}=Matrix{T}(undef, 0, 0);
                         inputgram::Bool=false,
                         sumtoone::Bool=false
                         ) where T <: AbstractFloat
    # If not provided, precompute Gram matrices
    WtW = inputgram ? W : W' * W
    WtX = inputgram ? X : W' * X
    # Init constants
    r, n = size(WtX)
    # If not provided, init H full of zeros
    H = length(H0) == 0 ? zeros(r, n) : H0
    # Solve the n NNLS subproblems with activeset
    for j in 1:n
        H[:,j] .= activeset(WtW, WtX[:,j], H[:,j], sumtoone=sumtoone)
    end
    # Return
    return H
end



function snpa(X::AbstractMatrix{T},
              r::Integer,
              epsilon::Float64=10e-9;
              normalize::Bool=false) where {T<:AbstractFloat}
    # Get dimensions
    m, n = size(X)

    # Optionally normalize so that columns of X sum to one
    if normalize
        X = copy(X) # copy in local scope to avoid modifying input
        for col in eachcol(X)
            col ./= sum(col)
        end
    end

    # Init
    # Set of selected indices
    K = zeros(Int, r)
    # Norm of columns of input X
    normX0 = sum.(abs2,eachcol(X))
    # Max of the columns norm
    nXmax = maximum(normX0)
    # Init residual
    normR = copy(normX0)
    # Init set of extracted columns
    U = zeros(T, m, r)
    # Init output H
    H = zeros(T, r, n)

    # Update intermediary variables (save time for norm computations)
    XtUK = zeros(T, n, r) # X^T * U(:,K)
    UKtUK = zeros(T, r, r) # U(:,K)^T * U(:,K)

    # SNPA loop
    i = 1
    while i <= r && sqrt(maximum(normR)/nXmax) > epsilon
        # Select column of X with largest l2-norm
        a = maximum(normR)
        # Check ties up to 1e-6 precision
        b = findall((a .- normR) / a .<= 1e-6)
        # In case of a tie, select column with largest norm of the input matrix
        _, d = findmax(normX0[b])
        b = b[d]
        # Save index of selected column, and column itself
        K[i] = b
        U[:,i] .= X[:,b]
        # Update XtUK
        XtUK[:,i] .= X'*U[:,i]
        # Update UKtUK
        if i == 1
            UKtUK[1,1] = U[:,i]' * U[:,i]
        else
            UtUi = U[:,1:i-1]' * U[:,i]
            UKtUK[1:i-1,i] .= UtUi
            UKtUK[i,1:i-1] .= UtUi
            UKtUK[i,i] = U[:,i]' * U[:,i]
        end
        # Update residual, that is solve
        # min_Y ||X - X[:,J] Y||
        # TODO add constraint that Y^T * e <= e with e vec of ones
        H[1:i,:] .= matrixactiveset(X, X[:,K[1:i]], H[1:i,:])
        # Update the norm of the columns of the residual
        normR .= (normX0- 2 * sum.(eachcol(XtUK[:,1:i]' .* H[1:i,:]))+ sum.(eachcol(H[1:i,:] .* (UKtUK[1:i,1:i] * H[1:i,:]))))
        # Increment iterator
        i += 1
    end

    return K
end

